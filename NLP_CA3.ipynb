{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import treebank\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import LSTM, GRU, Dense, Input, Dropout, Bidirectional, Masking, Embedding, SimpleRNN, TimeDistributed, RNN, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk.tree import Tree\n",
    "from nltk.chunk import tree2conlltags, conlltags2tree\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tensorflow.keras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('universal_tagset')\n",
    "# nltk.download('tagsets')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640147b1",
   "metadata": {},
   "source": [
    "## Part 1: Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92287699",
   "metadata": {},
   "source": [
    "## الف"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca0bb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(nltk.help.upenn_tagset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f76ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBPOSLoader:\n",
    "    # *الف*\n",
    "    def __init__(self, set_tagset = True, test_portion = 0.1, validation_portion = 0.1):\n",
    "        if set_tagset:\n",
    "            self.ptb = list(treebank.tagged_sents(tagset='universal'))\n",
    "        else:\n",
    "            self.ptb = list(treebank.tagged_sents())\n",
    "            \n",
    "        self.test_portion = test_portion\n",
    "        self.validation_portion = validation_portion\n",
    "        self.split_train_val_test_set()\n",
    "            \n",
    "    # *ب* \n",
    "    def split_train_val_test_set(self):\n",
    "        self.train_set, self.test_set = train_test_split(self.ptb, test_size = self.test_portion, random_state = 100)\n",
    "        self.train_set, self.val_set = train_test_split(self.train_set, test_size = self.validation_portion, random_state = 100)\n",
    "        return self.train_set, self.val_set, self.test_set\n",
    "        \n",
    "    def extract_all_words_tag(self):\n",
    "        self.train_words_tag = [word_tag for record in self.train_set for word_tag in record]\n",
    "        self.val_words_tag = [word_tag for record in self.val_set for word_tag in record]\n",
    "        self.test_words_tag = [word_tag for record in self.test_set for word_tag in record]\n",
    "        return self.train_words_tag, self.val_words_tag, self.test_words_tag\n",
    "        \n",
    "    def set_vocab_and_tagset(self):\n",
    "        vocab = set([word_tag[0] for word_tag in self.train_words_tag])  \n",
    "        tagset = sorted(list(set([pair[1] for pair in self.train_words_tag])))\n",
    "        return vocab, tagset\n",
    "        \n",
    "        \n",
    "class POSTagger:\n",
    "    def __init__(self, dataset_loader, smoothing_const = 0.001):\n",
    "#         nltk.download('universal_tagset')\n",
    "#         nltk.download('tagsets')\n",
    "#         nltk.download('punkt')\n",
    "#         nltk.download('averaged_perceptron_tagger')\n",
    "#         nltk.download('maxent_ne_chunker')\n",
    "#         nltk.download('words')\n",
    "        \n",
    "        self.train_set, self.val_set, self.test_set = dataset_loader.split_train_val_test_set()\n",
    "        \n",
    "        self.emission_file_name = 'emission_ptb.csv'\n",
    "        self.smoothing_const = smoothing_const\n",
    "        self.train_words_tag, self.val_words_tag, self.test_words_tag = dataset_loader.extract_all_words_tag()\n",
    "        self.vocab, self.tagset = dataset_loader.set_vocab_and_tagset()\n",
    "        self.t = len(self.tagset)\n",
    "        self.v = len(self.vocab)\n",
    "        \n",
    "        self.test_run_base = [tup for sent in self.test_set for tup in sent]\n",
    "        self.test_tagged_words = [tup[0] for sent in self.test_set for tup in sent]\n",
    "        \n",
    "    def get_transition(self, curr_tag, prev_tag):\n",
    "        count_prev_tag = len([tag for tag in self.all_tags if tag == prev_tag])\n",
    "        count_prev_tag_curr_tag = 0\n",
    "        # count_prev_tag_curr_tag = len([tagsets[i] for i in range(len(tagsets)-1) if (tagsets[i + 1] == curr_tag and tagsets[i] == prev_tag)])\n",
    "        for i in range(len(self.all_tags)-1):\n",
    "            if self.all_tags[i + 1] == curr_tag and self.all_tags[i] == prev_tag:\n",
    "                count_prev_tag_curr_tag += 1\n",
    "        return count_prev_tag_curr_tag / count_prev_tag\n",
    "    \n",
    "    def set_transition_matrix(self, print_transition_matrix = False):\n",
    "        self.all_tags = [pair[1] for pair in self.train_words_tag]\n",
    "        self.tags_matrix = np.zeros((len(self.tagset), len(self.tagset)), dtype='float32')\n",
    "        for i, t1 in enumerate(list(self.tagset)):\n",
    "            for j, t2 in enumerate(list(self.tagset)): \n",
    "                self.tags_matrix[i, j] = self.get_transition(t2, t1)\n",
    "        self.trans_df = pd.DataFrame(self.tags_matrix, columns = list(self.tagset), index=list(self.tagset))\n",
    "        if print_transition_matrix:\n",
    "            print(self.trans_df)\n",
    "        \n",
    "    # Calculating Emission Probabilities\n",
    "    # The B emission probabilities, $P(wi|ti)$, represent the probability, given a tag (say Verb),\n",
    "    # that it will be associated with a given word.\n",
    "    def get_emission(self, word, tag):\n",
    "        pair_with_tag_t = [pair for pair in self.train_words_tag if pair[1] == tag]    \n",
    "        word_with_tag_t = [pair[0] for pair in pair_with_tag_t if pair[0] == word[0]]\n",
    "        return len(word_with_tag_t) / len(pair_with_tag_t)\n",
    "    \n",
    "    def set_emission_matrix(self, read_from_file = True, print_emission_matrix = False):\n",
    "        if read_from_file:\n",
    "            self.read_emission_csv()\n",
    "        else:\n",
    "            self.emission_matrix = np.zeros((len(self.test_words_tag), len(self.tagset)), dtype='float32')\n",
    "            for i, w in enumerate(list(self.test_words_tag)):\n",
    "                if i % 300 == 0:\n",
    "                    print(f\"round: {i}\")\n",
    "                for j, t2 in enumerate(list(self.tagset)): \n",
    "                    self.emission_matrix[i, j] = self.get_emission(w, t2)\n",
    "            self.emission_df = pd.DataFrame(self.emission_matrix, columns = list(self.tagset), index=list(self.test_words_tag))\n",
    "        \n",
    "        if print_emission_matrix:\n",
    "            print(self.emission_df)\n",
    "        \n",
    "    def save_emission_csv(self):\n",
    "        self.emission_df.to_csv(self.emission_file_name, index = list(self.test_words_tag))\n",
    "        \n",
    "    def read_emission_csv(self):\n",
    "        self.emission_df = pd.read_csv(self.emission_file_name).set_index('Unnamed: 0')\n",
    "\n",
    "    # *پ*\n",
    "    #not considering OOV words\n",
    "    def vanilla_viterbi(self, read_emission_file = True):\n",
    "        viterbi = []\n",
    "        for key, word in enumerate(self.test_tagged_words):\n",
    "            max_prob = -1\n",
    "            best_tag = None\n",
    "            for tag in self.tagset:\n",
    "                if key == 0:\n",
    "                    transition_p = self.trans_df.loc['.', tag]              \n",
    "                else:\n",
    "                    last_state = viterbi[-1]\n",
    "                    transition_p = self.trans_df.loc[last_state, tag]\n",
    "\n",
    "                emission_p = self.emission_df.iat[key, self.tagset.index(tag)]\n",
    "                \n",
    "                prob = emission_p * transition_p\n",
    "                if (prob >= max_prob):\n",
    "                    max_prob = prob\n",
    "                    best_tag = tag\n",
    "                    \n",
    "            viterbi.append(best_tag)\n",
    "        return list(zip(self.test_tagged_words, viterbi))\n",
    "    \n",
    "    # *پ* & *ث*\n",
    "    #considering OOV words\n",
    "    def smoothed_viterbi(self):\n",
    "        viterbi = []\n",
    "        for key, word in enumerate(self.test_tagged_words):\n",
    "            max_prob = -1\n",
    "            best_tag = None \n",
    "            for tag in self.tagset:\n",
    "                if !key:\n",
    "                    transition_p = self.trans_df.loc['.', tag]\n",
    "                else:\n",
    "                    last_state = viterbi[-1]\n",
    "                    transition_p = self.trans_df.loc[viterbi[-1], tag]\n",
    "\n",
    "                if (word in self.vocab):\n",
    "                    emission_p = self.emission_df.iat[key, self.tagset.index(tag)]\n",
    "                else:\n",
    "                    emission_p = self.smoothing_const\n",
    "\n",
    "                prob = emission_p * transition_p\n",
    "                if (prob >= max_prob):\n",
    "                    max_prob = prob\n",
    "                    best_tag = tag\n",
    "\n",
    "            viterbi.append(best_tag)\n",
    "        return list(zip(self.test_tagged_words, viterbi))\n",
    "    \n",
    "    def apply_viterbi(self, read_emission_matrix = True, smoothing = True):\n",
    "        self.set_transition_matrix()\n",
    "        self.set_emission_matrix(read_from_file = read_emission_matrix)\n",
    "        if smoothing:\n",
    "            tagged_res = self.smoothed_viterbi()\n",
    "        else:\n",
    "            tagged_res = self.vanilla_viterbi()\n",
    "        return tagged_res\n",
    "    \n",
    "    # *ت*\n",
    "    def evaluate(self, tagged_res):\n",
    "        res = [] \n",
    "        wrong_preds = []\n",
    "        for i, j in zip(tagged_res, self.test_run_base):\n",
    "            if i == j:\n",
    "                res.append(1)\n",
    "            \n",
    "        for _, pair in enumerate(zip(tagged_res, self.test_run_base)):\n",
    "            if pair[0]!=pair[1]:\n",
    "                wrong_preds.append(pair)\n",
    "                \n",
    "        accuracy = len(res)/len(tagged_res)\n",
    "        return accuracy, wrong_preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d37817",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_loader = PTBPOSLoader()\n",
    "pos_tagger = POSTagger(ptb_loader)\n",
    "viterbi_tag_res = pos_tagger.apply_viterbi()\n",
    "accuracy, wrong_preds = pos_tagger.evaluate(viterbi_tag_res)\n",
    "print(f\"accuracy = {accuracy}\")\n",
    "for i in range(10):\n",
    "    print(f\"word: {wrong_preds[i][0][0]} | predicted: {wrong_preds[i][0][1]} | expected: {wrong_preds[i][1][1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef03874",
   "metadata": {},
   "source": [
    "## ح & ج"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentPOSTagger:\n",
    "    def __init__(self):\n",
    "        def get_XY(dataset):\n",
    "            X = []\n",
    "            Y = []\n",
    "            for sentence in dataset:\n",
    "                X_sentence = []\n",
    "                Y_sentence = []\n",
    "                for entity in sentence:         \n",
    "                    X_sentence.append(entity[0])\n",
    "                    Y_sentence.append(entity[1])\n",
    "                X.append(X_sentence)\n",
    "                Y.append(Y_sentence)\n",
    "            return X, Y\n",
    "        \n",
    "        def get_num_of_words_and_tags(X, Y):\n",
    "            num_of_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
    "            num_of_tags = len(set([word.lower() for sentence in Y for word in sentence]))\n",
    "            return num_of_words, num_of_tags\n",
    "        \n",
    "        def verbose(records, num_words, num_tags):\n",
    "            print(\"Total number of tagged sentences: {}\".format(len(records)))\n",
    "            print(\"Vocabulary size: {}\".format(num_words))\n",
    "            print(\"Total number of tags: {}\".format(num_tags))\n",
    "            \n",
    "        ptb_pos_loader = PTBPOSLoader()\n",
    "        self.train_set, self.val_set, self.test_set = ptb_pos_loader.split_train_val_test_set()\n",
    "        ptb_pos_loader.extract_all_words_tag()\n",
    "        self.vocab, self.tagset = ptb_pos_loader.set_vocab_and_tagset()\n",
    "        self.t = len(self.tagset)\n",
    "        self.v = len(self.vocab)\n",
    "        \n",
    "        self.X, self.Y = get_XY(self.train_set)\n",
    "        self.X_val, self.Y_val = get_XY(self.val_set)\n",
    "        self.X_test, self.Y_test = get_XY(self.test_set)\n",
    "        \n",
    "        self.num_words, self.num_tags = get_num_of_words_and_tags(self.X, self.Y)\n",
    "        self.num_words_val, self.num_tags_val = get_num_of_words_and_tags(self.X_val, self.Y_val)\n",
    "        self.num_words_test, self.num_tags_test = get_num_of_words_and_tags(self.X_test, self.Y_test)\n",
    "        \n",
    "#         verbose(self.X, self.num_words, self.num_tags)\n",
    "#         verbose(self.X, self.num_words_val, self.num_tags_val)\n",
    "#         verbose(self.X, self.num_words_test, self.num_tags_test)\n",
    "        \n",
    "    def initialize_tokenizer(self, data):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(data)   \n",
    "        encoded = word_tokenizer.texts_to_sequences(data) \n",
    "        return encoded\n",
    "    \n",
    "    def apply_padding(self, tokenized_x, tokenized_y):\n",
    "        pad_x = pad_sequences(tokenized_x, maxlen = self.max_length, padding = \"pre\", truncating = \"post\")\n",
    "        pad_y = pad_sequences(tokenized_y, maxlen = self.max_length, padding = \"pre\", truncating = \"post\")\n",
    "        return pad_x, pad_y\n",
    "    \n",
    "    def prepare_xy(self, padding_max_length = 100):\n",
    "        self.max_length = padding_max_length\n",
    "        tokenized_X = self.initialize_tokenizer(self.X)\n",
    "        tokenized_X_val = self.initialize_tokenizer(self.X_val)\n",
    "        tokenized_X_test = self.initialize_tokenizer(self.X_test)\n",
    "        tokenized_Y = self.initialize_tokenizer(self.Y)\n",
    "        tokenized_Y_val = self.initialize_tokenizer(self.Y_val)\n",
    "        tokenized_Y_test = self.initialize_tokenizer(self.Y_test)\n",
    "        \n",
    "        self.X, self.Y = self.apply_padding(tokenized_X, tokenized_Y)\n",
    "        self.X_val, self.Y_val = self.apply_padding(tokenized_X_val, tokenized_Y_val)\n",
    "        self.X_test, self.Y_test = self.apply_padding(tokenized_X_test, tokenized_Y_test)\n",
    "        self.Y, self.Y_val, self.Y_test = to_categorical(self.Y), to_categorical(self.Y_val), to_categorical(self.Y_test)\n",
    "        self.num_classes = self.Y.shape[2]\n",
    "        \n",
    "    def make_model(self, layer_type = \"RNN\", num_of_units = 128, embedding_size = 300):\n",
    "        self.rnn_model = Sequential()\n",
    "        self.rnn_model.add(Embedding(input_dim = self.v, output_dim = embedding_size, input_length = self.max_length, trainable = False))\n",
    "        if layer_type == \"RNN\":\n",
    "            self.rnn_model.add(SimpleRNN(num_of_units, return_sequences = True, dropout = 0.1, recurrent_dropout = 0.1))\n",
    "        elif layer_type == \"LSTM\":\n",
    "            self.rnn_model.add(LSTM(num_of_units, return_sequences = True, dropout = 0.1, recurrent_dropout = 0.1))\n",
    "        elif layer_type == \"GRU\":\n",
    "            self.rnn_model.add(GRU(num_of_units, return_sequences = True, dropout = 0.1, recurrent_dropout = 0.1))\n",
    "        self.rnn_model.add(TimeDistributed(Dense(self.num_classes, activation='softmax')))\n",
    "        \n",
    "        self.rnn_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])\n",
    "        print(self.rnn_model.summary())\n",
    "        \n",
    "    def fit_model(self, test_data = False, batch_size = 32, epochs = 10):\n",
    "        if test_data:\n",
    "            rnn_training = self.rnn_model.fit(self.X, self.Y, batch_size=batch_size, epochs=epochs, validation_data=(self.X_test, self.Y_test))\n",
    "        else:\n",
    "            rnn_training = self.rnn_model.fit(self.X, self.Y, batch_size=batch_size, epochs=epochs, validation_data=(self.X_val, self.Y_val))\n",
    "        return rnn_training\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e69470",
   "metadata": {},
   "source": [
    "### Effect of Hyperparameters on Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01bcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, accuracies, topic):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(values))\n",
    "    fig.set_figheight(3)\n",
    "    fig.set_figwidth(15)\n",
    "    for i in range(len(values)):\n",
    "        ax[i].plot(accuracies[i].history[\"accuracy\"], label = \"Train\")\n",
    "        ax[i].plot(accuracies[i].history[\"val_accuracy\"], label =\"Test\")\n",
    "        ax[i].set_title(f'{topic} Size: {values[i]}\\nval_accuracy={accuracies[i].history[\"val_accuracy\"][-1]:.5f}')\n",
    "        ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f\"{topic}.png\")\n",
    "    \n",
    "def plot_parapeters(models):\n",
    "    plt.style.use('seaborn')\n",
    "\n",
    "    batches = [16, 32, 64]\n",
    "    pad_max_lengths = [50, 100]\n",
    "    embedding_sizes = [100, 200, 300]\n",
    "    units = [32, 64, 128]\n",
    "\n",
    "\n",
    "    for model in models:\n",
    "        print(\"MODEL:\", model)\n",
    "        batch_accuracies = dict()\n",
    "        max_len_accuracies = dict()\n",
    "        embedding_size_accuracy = dict()\n",
    "\n",
    "        for i, sz in enumerate(embedding_sizes):\n",
    "            rnn_model = RecurrentPOSTagger()\n",
    "            rnn_model.prepare_xy(padding_max_length = 100)\n",
    "            rnn_model.make_model(layer_type = model, num_of_units = 128, embedding_size = sz)\n",
    "            hist = rnn_model.fit_model(batch_size = 32, epochs = 15)\n",
    "            embedding_size_accuracy[i] = hist\n",
    "\n",
    "        plot(embedding_sizes, embedding_size_accuracy, f\"{model}_Embedding\")\n",
    "\n",
    "        for i, sz in enumerate(pad_max_lengths):\n",
    "            rnn_model = RecurrentPOSTagger()\n",
    "            rnn_model.prepare_xy(padding_max_length = sz)\n",
    "            rnn_model.make_model(layer_type = model, num_of_units = 128, embedding_size = 300)\n",
    "            hist = rnn_model.fit_model(batch_size = 32, epochs = 15)\n",
    "            max_len_accuracies[i] = hist\n",
    "\n",
    "        plot(pad_max_lengths, max_len_accuracies, f\"{model}_MaxLen\")\n",
    "\n",
    "        for i, sz in enumerate(batches):\n",
    "            rnn_model = RecurrentPOSTagger()\n",
    "            rnn_model.prepare_xy(padding_max_length = 100)\n",
    "            rnn_model.make_model(layer_type = model, num_of_units = 128, embedding_size = 300)\n",
    "            hist = rnn_model.fit_model(batch_size = sz, epochs = 15)\n",
    "            batch_accuracies[i] = hist\n",
    "\n",
    "        plot(batches, batch_accuracies, f\"{model}_Batch\")\n",
    "\n",
    "models = [\"LSTM\", \"GRU\"]    \n",
    "plot_parapeters(models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a5638",
   "metadata": {},
   "source": [
    "### Studing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f41301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(hist, topic):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(hist.history[\"accuracy\"], label = \"Train\")\n",
    "    plt.plot(hist.history[\"val_accuracy\"], label =\"Test\")\n",
    "    plt.title(\"Accuracy of Train and validation Data\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    fig.savefig(f\"{topic}.png\")\n",
    "\n",
    "units = [32, 64, 128]\n",
    "batches = [16, 32, 64]\n",
    "pad_max_lengths = [50, 100]\n",
    "embedding_sizes = [100, 300]\n",
    "models = [\"RNN\", \"LSTM\", \"GRU\"]\n",
    "\n",
    "f = open(\"tuning_results.txt\", \"a\")\n",
    "\n",
    "for model in models:\n",
    "    f.write(f\"--------------> LAYER TYPE: {model} <--------------\\n\")\n",
    "    for batch in batches:\n",
    "        for max_length in pad_max_lengths:\n",
    "            for embedding_size in embedding_sizes:\n",
    "                rnn_model = RecurrentPOSTagger()\n",
    "                rnn_model.prepare_xy(padding_max_length = max_length)\n",
    "                rnn_model.make_model(layer_type = model, num_of_units = 64, embedding_size = embedding_size)\n",
    "                hist = rnn_model.fit_model(batch_size = batch, epochs = 10)\n",
    "                f.write(f'Layer: {model}, Batch size: {batch}, Padding Max length: {max_length}, Embedding size: {embedding_size}\\n')\n",
    "                f.write(f'val_accuracy={hist.history[\"val_accuracy\"][-1]:.5f}\\n')\n",
    "                f.write('-'*30)\n",
    "                f.write('\\n')\n",
    "                \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c0bf0",
   "metadata": {},
   "source": [
    "### Examining effect of 3 different unit values on RNN, LSTM and GRU : 32, 64, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab302d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in [\"RNN\", \"LSTM\", \"GRU\"]:\n",
    "    for unit in [32, 64, 128]:\n",
    "        rnn_model = RecurrentPOSTagger()\n",
    "        rnn_model.prepare_xy(padding_max_length = 100)\n",
    "        rnn_model.make_model(layer_type = layer, num_of_units = unit, embedding_size = 300)\n",
    "        hist = rnn_model.fit_model(batch_size = batch, epochs = 15)\n",
    "        print(f'accuracy: {hist.history[\"accuracy\"][-1]:.5f}')\n",
    "        print(f'val accuracy: {hist.history[\"val_accuracy\"][-1]:.5f}')\n",
    "        plot(hist, f\"layer_{layer}_unit_{unit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170205da",
   "metadata": {},
   "source": [
    "## چ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6abdf2",
   "metadata": {},
   "source": [
    "## Testing Model on Test dataset\n",
    "\n",
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model_test_ziad = RecurrentPOSTagger()\n",
    "rnn_model_test_ziad.prepare_xy(padding_max_length = 100)\n",
    "rnn_model_test_ziad.make_model(\"RNN\", num_of_units = 128, embedding_size = 300)\n",
    "hist = rnn_model_test_ziad.fit_model(test_data=True, batch_size = 64, epochs = 12)\n",
    "print(f'accuracy: {hist.history[\"accuracy\"][-1]:.5f}')\n",
    "plot(hist, f\"test_RNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18294ba6",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cee4a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rnn_model_test_ziad = RecurrentPOSTagger()\n",
    "rnn_model_test_ziad.prepare_xy(padding_max_length = 100)\n",
    "rnn_model_test_ziad.make_model(\"LSTM\", num_of_units = 64, embedding_size = 300)\n",
    "hist = rnn_model_test_ziad.fit_model(test_data=True, batch_size = 32, epochs = 12)\n",
    "print(f'accuracy: {hist.history[\"accuracy\"][-1]:.5f}')\n",
    "plot(hist, f\"test_RNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb4029",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89500f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model_test_ziad = RecurrentPOSTagger()\n",
    "rnn_model_test_ziad.prepare_xy(padding_max_length = 100)\n",
    "rnn_model_test_ziad.make_model(\"GRU\", num_of_units = 64, embedding_size = 300)\n",
    "hist = rnn_model_test_ziad.fit_model(test_data=True, batch_size = 32, epochs = 12)\n",
    "print(f'accuracy: {hist.history[\"accuracy\"][-1]:.5f}')\n",
    "plot(hist, f\"test_RNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14693cc",
   "metadata": {},
   "source": [
    "## Part 2: Named entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5235489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class NER:\n",
    "    # *الف*\n",
    "    def __init__(self, smoothing_const = '0.001'):              \n",
    "        self.emission_file_name = 'NER_emission_15.csv'\n",
    "        self.smoothing_const = smoothing_const\n",
    "\n",
    "        self.ptb_dataset = treebank.tagged_sents()  \n",
    "        self.word_ne = []\n",
    "        for record in self.ptb_dataset:\n",
    "            tree = nltk.ne_chunk(record)\n",
    "            iob_tags = tree2conlltags(tree)\n",
    "            tags = [pair[2] for pair in iob_tags]\n",
    "            self.word_ne = self.word_ne + [(i[0], i[2]) for i in iob_tags]\n",
    "        \n",
    "        self.train_set, self.test_set = train_test_split(self.word_ne, test_size = 0.15, random_state = 100)\n",
    "\n",
    "        self.rec_ne_test_run_base = [sent for sent in self.test_set]\n",
    "        self.rec_ne_test_tagged_words = [sent[0] for sent in self.test_set]\n",
    "        \n",
    "        self.words_tag = [record for record in self.train_set]      \n",
    "        self.ne_vocab = set([word_tag[0] for word_tag in self.word_ne])\n",
    "\n",
    "        self.all_tags = [record[1] for record in self.train_set]\n",
    "        self.ne_tagsets = sorted(list(set(self.all_tags)))\n",
    "\n",
    "        self.ne_t = len(self.ne_tagsets)\n",
    "        self.ne_v = len(self.ne_vocab)\n",
    "        \n",
    "    def save_emission_csv(self):\n",
    "        self.rec_ne_emission_df.to_csv(self.emission_file_name, index = list(self.test_set))\n",
    "        \n",
    "    def read_emission_csv(self):\n",
    "        self.rec_ne_emission_df = pd.read_csv(self.emission_file_name).set_index('Unnamed: 0')\n",
    "    # *ب*\n",
    "    def get_transition(self, curr_tag, prev_tag):\n",
    "        count_prev_tag = len([tag for tag in self.all_tags if tag == prev_tag])\n",
    "        count_prev_tag_curr_tag = 0\n",
    "        for i in range(len(self.all_tags)-1):\n",
    "            if self.all_tags[i + 1] == curr_tag and self.all_tags[i] == prev_tag:\n",
    "                count_prev_tag_curr_tag += 1\n",
    "        return count_prev_tag_curr_tag / count_prev_tag\n",
    "    \n",
    "    def set_transition_matrix(self, print_transition_matrix = False):\n",
    "        self.ne_tags_matrix = np.zeros((len(self.ne_tagsets), len(self.ne_tagsets)), dtype='float32')\n",
    "        for i, t1 in enumerate(list(self.ne_tagsets)):\n",
    "            for j, t2 in enumerate(list(self.ne_tagsets)): \n",
    "                self.ne_tags_matrix[i, j] = self.get_transition(t2, t1)\n",
    "                if \"I-\" in t2:\n",
    "                    if (\"B-\" in t1 and t1[2:] != t2[2:]) or (\"I-\" in t1 and t1[2:] != t2[2:]) or \"O\" in t1:\n",
    "                        transition_p = 0\n",
    "\n",
    "        self.ne_trans_df = pd.DataFrame(self.ne_tags_matrix, columns = list(self.ne_tagsets), index=list(self.ne_tagsets))\n",
    "        if print_transition_matrix:\n",
    "            print(self.ne_trans_df)\n",
    "        \n",
    "    def get_emission(self, word, tag):\n",
    "        pair_with_tag_t = [pair for pair in self.train_set if pair[1] == tag]    \n",
    "        word_with_tag_t = [pair[0] for pair in pair_with_tag_t if pair[0] == word[0]]\n",
    "        return len(word_with_tag_t) / len(pair_with_tag_t)\n",
    "    \n",
    "    def set_emission_matrix(self, read_from_file = True, print_emission_matrix = False):\n",
    "        if read_from_file:\n",
    "            self.read_emission_csv()\n",
    "        else:\n",
    "            self.rec_ne_emission_matrix = np.zeros((len(self.test_set), len(self.ne_tagsets)), dtype='float32')\n",
    "            for i, w in enumerate(list(self.test_set)):\n",
    "                if i % 300 == 0:\n",
    "                    print(f\"round: {i}\")\n",
    "                for j, t2 in enumerate(list(self.ne_tagsets)):\n",
    "                    self.rec_ne_emission_matrix[i, j] = self.ne_get_emission(w, t2)\n",
    "            self.rec_ne_emission_df = pd.DataFrame(self.rec_ne_emission_matrix, columns = list(self.ne_tagsets), index=list(self.test_set))\n",
    "\n",
    "        if print_emission_matrix:\n",
    "            print(self.rec_ne_emission_df)\n",
    "      \n",
    "    # *ب*\n",
    "    def Viterbi_NER(self):\n",
    "        state = []\n",
    "        for key, word in enumerate(self.rec_ne_test_tagged_words):\n",
    "            max_prob = -1\n",
    "            best_tag = None\n",
    "            for tag in self.ne_tagsets:\n",
    "                if key == 0:\n",
    "                    transition_p = self.ne_trans_df.loc['O', tag]\n",
    "                else:\n",
    "                    transition_p = self.ne_trans_df.loc[state[-1], tag]\n",
    "\n",
    "                if (word in self.ne_vocab):\n",
    "                    emission_p = self.rec_ne_emission_df.iat[key, self.ne_tagsets.index(tag)]\n",
    "                else:\n",
    "                    emission_p = self.smoothing_const\n",
    "\n",
    "                prob = emission_p * transition_p\n",
    "                if (prob >= max_prob):\n",
    "                    max_prob = prob\n",
    "                    best_tag = tag\n",
    "\n",
    "            state.append(best_tag)\n",
    "        return list(zip(self.rec_ne_test_tagged_words, state))\n",
    "    \n",
    "    def apply_viterbi(self, read_emission_matrix = True):\n",
    "        self.set_transition_matrix()\n",
    "        self.set_emission_matrix(read_from_file = read_emission_matrix)\n",
    "        tagged_seq_ne = self.Viterbi_NER()\n",
    "        return tagged_seq_ne\n",
    "\n",
    "    # *پ*\n",
    "    def evaluate(self, tagged_res):\n",
    "        res = [] \n",
    "        for i, j in zip(tagged_res, self.test_run_base):\n",
    "            if i == j:\n",
    "                res.append(1)\n",
    "        accuracy = len(res)/len(tagged_res)\n",
    "        report = classification_report([x[1] for x in self.rec_ne_test_run_base], [y[1] for y in tagged_res])\n",
    "        return accuracy, report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c4a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = NER()\n",
    "viterbi_ner_res = ner.apply_viterbi()\n",
    "accuracy, report = ner.evaluate(viterbi_ner_res)\n",
    "print(f\"accuracy = {accuracy}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4bdbb7",
   "metadata": {},
   "source": [
    "### Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643096fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tags_matrix = np.zeros((len(ne_tagsets), len(ne_tagsets)), dtype='float32')\n",
    "for i, t1 in enumerate(list(ne_tagsets)):\n",
    "    for j, t2 in enumerate(list(ne_tagsets)): \n",
    "        ne_tags_matrix[i, j] = ne_get_transition(t2, t1, train_set)[0]/ne_get_transition(t2, t1, train_set)[1]\n",
    "ne_trans_df = pd.DataFrame(ne_tags_matrix, columns = list(ne_tagsets), index=list(ne_tagsets))\n",
    "ne_trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a0bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
